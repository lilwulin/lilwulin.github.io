---
layout: post
title: Log-Structured Merge-Tree 日志结构合并树
categories: blog
---

**[日志结构合并树(Log-Structured Merge-Tree)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.44.2782&rep=rep1&type=pdf)**是一种充分利用硬盘顺序操作以提高IO性能的数据结构，在1996年被Patrick O'Neil等人发明出来之后，已经被广泛应用于各种数据库中，比如HBase，Google Bigtable和LevelDB等等。在谈LSM树之前，我们先来回顾一下B+树，看看这个老生常谈的数据结构有什么问题，然后我们再看LSM树是如何解决这个问题的。

## B+树
B+树常常被用于各种RDBMS的索引。它的大致结构如下图：

![]({{ site.baseurl }}/images/BpTree.png)

图中的Page可以看成是硬盘映射到内存的一个block。各个Page组成了一个具有良好查找性能的B+树，允许在大致**O(Log N)**的复杂度下找到对应的Key。但同时B+树又有一个特点，那就是各个Page就算逻辑上相隔很近，但物理上可能相隔很远。这导致了B+树进行写操作时，Page的分裂或者合并，会造成大量的**随机**硬盘读写。如果我们的应用场景有大量的写需求，不断地对B+树中的键值对进行增删无疑会伤害到IO性能。以下这幅图来自ACM杂志2009年的一则[报告](http://queue.acm.org/detail.cfm?id=1563874)：

![](http://deliveryimages.acm.org/10.1145/1570000/1563874/jacobs3.jpg)

可以看到，无论是机械还是固态硬盘，顺序读写都会比随机读写要快2~3倍以上。因此，为了提高IO性能，我们需要一种数据结构，它能让我们在对索引进行修改的时候，对硬盘进行顺序操作而不是随机操作，使得我们的写操作性能大幅提升，代价就是在可以接受的范围内牺牲一点读性能。这就是**日志结构合并树(Log-Structured Merge-Tree)**的作用。

## 日志结构合并树(Log-Structured Merge-Tree)
我们来了解一下LSM树的基础原理，然后我们再在基础原理上做一些修改和增加，以达到提升性能的目的。

LSM树有许多不同的实现方法，但是背后的思想大致相同。它的结构总体可以分为两部分，一部分是内存中的Index，另一部分是硬盘中已经排序好的文件。我们可以画出下面的简略图：

![]({{ site.baseurl }}/images/LSMTreeStructure.png)

LSM树在内存中会维护着一个类似于树形的索引，图中绿色的二叉树只是示意，它可以是红黑树或者其他树，甚至是一个[skip list](https://en.wikipedia.org/wiki/Skip_list)。假设我们的LSM树用来存储键值对，我们可以总结出写操作的步骤：

**写操作**：增删改的操作被写入内存中的树索引；如果树大小达到某个程度，我们把它flush到磁盘上，作为一个新的文件，因为我们内存中使用的是树结构，因此我们很容易就可以输出对于Key排序好的文件。

**读操作**：当我们要读一个键值对的时候，我们先查看内存中的Index，因为最新的操作总会先到内存中。如果我们没有找到，那我们就得访问磁盘，从最新的文件一直找到最旧的文件。因为每个文件都是排序好的，所以我们可以用二分查找等方法找到我们想要的键值对。

这样一来，由于写操作的时候不涉及随机硬盘操作，IO性能有了较大的提高。但是我们还得想办法优化一下读操作的性能，毕竟如果我们的数据量上去之后，遍历所有的文件确实够呛。

### 优化#1 - 文件索引
我们在刚开头的时候提到page。在这里，我们可以把每个文件切分成一个个page，然后在文件头部或者尾部维护一组元信息，标明每个page的offset和涵盖的Key范围。这样，我们在查找的时候，可以查看这组元信息，然后直接跳到对应的page进行查找。这样有一个好处，就是我们的键值对不需要定长。我们还可以把这些page索引保存在内存中，这样不需要读取文件里的元信息就可以直接跳到对应的page。

### 优化#2 - [Bloom Filter](https://en.wikipedia.org/wiki/Bloom_filter)
Bloom filter可以告诉我们一个Key是否存在。它的原理很简单：

**插入**：首先我们维护着一个长度为L的bit vector#1，每加入一个key，我们把这个key哈希成长度同样为L的bit vector#2，***或***上原来的bit vector#1。

**查找**：当我们要查看一个key是否存在的时候，我们同样把它哈希成长度为L的bit vector#2，***与***上bloom filter的bit vector#1，如果结果和bit vector#2相等，那就返回true，否则返回false。

我们可以总结成一幅图：

![]({{ site.baseurl }}/images/BloomFilter.png)

这样一来，虽然有一定几率就算key不存在，我们也会得到true，但是当我们得到false的时候，key就一定不存在。冲突的概率在维基百科链接里有很详细的推论，这里就不赘述了。我们可以把Bloom filter应用到每个文件，这样一来我们就有一定概率知道key一定不存在某些文件中，而且注意，我们只增加了很少的空间，对于每个文件只多了长度为L的bit vector。

### 文件合并
前面一些优化，说到底都是一些小技巧，我们必须从根本上解决文件过多的问题。因此，我们要定期地对文件进行合并，由于原来的文件对于Key已经排序好了，我们可以通过类似外排的算法再次合并出排序好的更大的文件。

我们先从简单出发，首先想一个最简单的策略，就是当文件数量达到一定程度的时候，我们可以将这些文件合并成一个更大的文件。由于我们内存中的树索引大小有固定的阈值，flush出来的这些文件都有差不多的大小，这些文件又可以合成**更大的文件**，更大的文件数量达到一定程度又可以合成**更更大的文件**，一直循环下去，每次合并的时候我们对于每个键值对都只保留最新的操作，以达到去重的目的。于是我们很容易得到一个分层(Level)的结构，如图：

![]({{ site.baseurl }}/images/LevelCompaction.png)

现在我们有了文件合并，但是上面的策略太过简单。每个文件都有对于Key的范围，有可能多个文件的范围会重合。因此，当我们需要查找某个键值对的时候，需要遍历多个文件。目前许多数据库使用另外一种新的策略，它能确保每一个level中，各个file之间Key的范围不会重合。LevelDB的Github上有一份详细的[说明](https://github.com/google/leveldb/blob/master/doc/impl.md)。这里简单介绍一下步骤：

 1. 当内存中树形索引达到一定大小，我们把它flush到磁盘形成一个Level 0的文件。
 2. Level 0的文件是特殊情况，我们允许它们之间有重合的Key范围。当Level 0的总大小达到一定程度，我们从中挑出一个文件（当然，为了腾出空间我们也可以挑出多个），和Level 1的文件做对比，看看有哪些文件和被挑出来的文件有重合，然后对它们进行合并操作，形成一个新的Level 1的文件。
 3. 同样，Level 1的总大小达到一定程度时，我们重复同样的步骤，即从Level 1到Level 2，Level 2到Level 3。

现在，采取了新的策略之后，我们需要查找的文件个数得到了减少，因为Level 0之后，对于一个Level里面，每个文件之间，都不会有重合的范围。但是由于我们每次只挑出一个文件，因此有可能会对于特定的写操作我们需要频繁地合并，产生额外的IO，这也是我们需要注意的取舍点。

## 总结
我们认识了日志结构合并树(Log-Structured Merge-Tree)；了解了它是如何通过顺序硬盘操作以提升IO性能；并且了解了它的几种优化方法及文件合并的策略。

## 参考
1. [http://www.benstopford.com/2015/02/14/log-structured-merge-trees/](http://www.benstopford.com/2015/02/14/log-structured-merge-trees)
2. [https://en.wikipedia.org/wiki/Log-structured_merge-tree](https://en.wikipedia.org/wiki/Log-structured_merge-tree)
3. [http://queue.acm.org/detail.cfm?id=1563874](http://queue.acm.org/detail.cfm?id=1563874)


















