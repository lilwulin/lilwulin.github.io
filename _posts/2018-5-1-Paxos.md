---
layout: post
title: 帕索斯岛上的议会：关于一致性的寓言
categories: blog
---

一个爱琴海上的小岛城邦，一群不务正业的议员和信使，是怎样通过一个协议使得议会能够正常运转的？这个情景来自于[ Leslie Lamport ](https://en.wikipedia.org/wiki/Leslie_Lamport)1989年写就的一篇著名论文[*The Part-Time Parliament*](https://lamport.azurewebsites.net/pubs/lamport-paxos.pdf)。这篇论文里介绍的Paxos协议，如今几乎成为了分布式系统一致性协议的同义词。在这篇博文里，我们一起了解一下什么是一致性问题，以及Paxos有趣的背景故事；然后再学习Paxos最简单的版本，最后通过扩展简单版本得到一个可以用于数据库等有状态进程的Paxos协议。


## 背景故事
最初的论文虚构了Paxos这样一个古希腊海岛城邦，以及介绍了以它名字命名的一致性协议。整个情景是这样的：因为要外出做生意，议会里面的议员们并非全职工作，他们可能待了一阵又要离开，或者要去休假，或者再也不回来（分布式节点宕机、重启）。议员们说话很小声，所以要把要说的话写在纸片上让信使传递给其他议员。信使同样也不务正业，可能中途离开去出海，个把月再回来，或者把信弄丢、送重复（网络包丢失、重复、超时）。各个议员手持一张羊皮纸，投票通过及记录法令（节点达成一致）。整个议会在Paxos的一群数学家设计的协议下正常运行。

论文中虚构的场景其实是在描述分布式系统中的**一致性问题（Consensus Problem）**。一个集合中的进程，都可以提出一个值。你可以将这个值看成是任何东西，对于Paxos议会来说，它是单独一条法令，对一个分布式数据库来说，它可以是单独一条log entry。最后集合中的进程需要达成共识，选定同一个值。这就是Paxos等一致性协议需要解决的问题。一个分布式系统可以通过一致性协议，将状态变化复制到多个节点上，这样节点在重启、结束、网络中断等意外发生的时候，其他节点可以继续运行，系统得以实现**高可用（High Availability）**。

*(The Part-Time Parliament的发表也是一个有趣的故事，Leslie Lamport本人记录下了这段[野史](http://lamport.azurewebsites.net/pubs/pubs.html#lamport-paxos)。作者在论文里假装考古学家，借用希腊字母和夹杂着几分冷幽默，使得后来很多人抱怨这篇论文晦涩难懂，于是在2001年Leslie又发表了[ Paxos Made Simple](https://lamport.azurewebsites.net/pubs/paxos-simple.pdf)。第二篇论文直截了当地描述了Paxos的运作过程，展示了Paxos原本的简洁设计，不过也丢失了一些趣味性和数学证明。)*

## Single-Decree Paxos
在介绍完整的Paxos之前，我们先了解一下最简单的Single-Decree Paxos，也就是很多博客提到的Basic Paxos。Decree就是指原来 *The Part-Time Parliament* 里Paxos议会用的法令。我们可以假设整个分布式系统只需要维护一个变量。这个版本的Paxos只要做一件事，就是为这个变量选定一个值。

在 Paxos 中，进程可以扮演三种角色：**Proposer**，**Acceptor**，**Learner**。为了解释方便，这篇博文只会使用到Proposer和Acceptor两种角色。顾名思义，Proposer的工作是向所有进程提出一个值；Acceptor的工作是接受Proposer提出的值。如果Proposer提出来的值被大多数进程接受（accept）了，那我们就说这个值被选定（chosen）了z。

我们的大多数是指至少超过半数。这样的话，小于一半的进程运行失败了，我们还会有至少一个进程保留着正确的值。这意味着Paxos的集群中如果有5个节点，那么最多可以有2个节点失败。如果有7个节点，那么最多可以有3个节点失败。如图所示，假设一个7节点的集群，3个数据节点被闪电击中，还有一个节点保留着数据（注意Proposer同时也可以扮演Acceptor的角色）：

![]({{ site.baseurl }}/images/NodeFailure.png)
**Figure 1: Paxos节点运行失败**

一个值如果被chosen，那么我们分布式系统维护的变量就只能选定这个值，不会再做出改变。选定之后不能再做出改变的原因是为了保证数据的**一致性（Consistency）**。因为我们需要保证高可用但同时需要节点能继续运行，所以我们只需要超过半数的节点同意一个值。如果我们允许变量的值可以变化，假如第一次proposal，超过半数的节点同意一个值；Proposer节点运行失败，一个没有参与到之前proposal的节点被选举为Proposer。第二次proposal，另外一些超过半数的节点同意另外一个值。这就像银行的数据集群，一个节点显示账户有100元，另外一个节点显示账户有200元，我们没办法判断哪一个才是正确的。如图所示：

![]({{ site.baseurl }}/images/PaxosMutableValue.png)
**Figure 2: 节点存在多个值**

为了避免这种混淆的情况，Paxos 规定了值被选定了之后，就不能再变化。同时，Proposer每一次提出proposal，都要捎带上一个顺序号。Paxos 通过proposal的顺序号来分辨发生的先后顺序。它的运行过程分为两个phase，过程如下：
1. Phase 1:
	<ol type="a" class="phase">
	<li>Proposer选定一个顺序号n，发给Acceptor们，叫他们准备处理proposal；</li>
	<li>Acceptor收到准备消息，对比n和当前收到过的<i>最大的顺序号m</i>。如果n比m要大，那Acceptor就告诉Proposer自己愿意处理这次的proposal，同时如果Acceptor自己维护的变量之前已经选定一个值，那Acceptor还会告诉Proposer自己变量的值，以及选定值时的proposal顺序号；如果n比m小，Acceptor可以选择无视Proposer，或者好心告诉Proposer：<i>“你的proposal已经过时了，这是我当前最大的proposal顺序号m。” </i>两种做法都不会影响Paxos的正确性，但是第二种做法能更快地让Proposer知道自己过时。</li>
	</ol>
2. Phase 2:
	<ol type="a" class="phase">
	<li>如果大多数的Acceptor答应Proposer，愿意处理proposal，那Proposer就把n和选定的值告诉参与proposal的Acceptor。如果Acceptor在Phase 1已经告诉了Proposer变量的当前值，那Proposer就没办法提出自己的值，只能使用Acceptor告诉自己的值。</li>
	<li>Acceptor收到proposal的顺序号n和Proposer提出的值v。因为Acceptor有可能已经收到其他顺序号更高的proposal，所以Acceptor要对比一下自己收到过的最大的顺序号m。如果n比m要小，Acceptor可以像Phase 1那样，告诉Proposer：<i>“你的proposal已经过时了，这是我当前最大的proposal顺序号m。” </i>否则，proposal就是具有当前最高的顺序号，也就是n，因此Acceptor将变量的值设置为v。</li>
	</ol>

过程听起来很复杂，其实可以用图和一些伪代码表示出来。我们先定义几组Proposer和Acceptor交流的接口：
1. `Prepare(n)`: Proposer通知Acceptor准备第n个proposal；
2. `Promise(n, (m, v))`: Acceptor回应愿意参与第n个proposal，并且告诉Proposer之前接受的最大的proposal顺序号m和对应的值v；
3. `Reject(m)`: Acceptor拒绝参与proposal，并且告诉Proposer自己接受过的最大的proposal顺序号m（可被用在两个Phase）；
4. `PleaseAccept(n, v)`: Proposer通知愿意参与第n次proposal的Acceptor接受对应的值v；
5. `OK()`: Acceptor回应确认第n次proposal已经被接受。

结合我们定义的接口，Paxos的过程如下图所示：

![]({{ site.baseurl }}/images/BasicPaxos.png)
**Figure 3: Paxos运行过程**

如果你知道二段提交（Two-Phase Commit），那你应该注意到Paxos就像打了鸡血的二段提交。不同之处是Paxos只需要大多数而不是全票通过；Phase 1和Phase 2之间不需要锁住变量；**到了Phase 2的时候Proposer才提出变量的值**。

## Multi-Paxos
仅仅维护一个变量是远远不够的，我们还会希望把Paxos运用到数据库或者其他形式的数据存储中。因此，我们需要Paxos能够记录一系列的状态变化。在 *Leslie Lamport* 的论文里，Paxos通过一个Log来实现记录状态变化的目的。每一个Log Entry都对应一个Paxos的实例。也就是说如果一个Log有10条记录，那就有10个Basic Paxos的实例，对应每一条记录。由于有多个Paxos同时运行，这个协议也叫 **Multi-Paxos**。Log里面的记录可以是一条数据库命令。我们可以有一条进程，在记录被选定（chosen）之后，执行对应的命令，碰到没被选定的记录时停下来等待。

当然，多个Paxos运行的时候，如果每个Paxos都要发一次Prepare请求，也是够浪费效率的。因此，我们可以做一些优化。我们可以注意到，Paxos只有到Phase 2的时候才会提出值。因此，当一个Proposer被选举出来的时候，它可以对每一条还没有被选定的记录对应的Paxos，都运行一次Phase 1，也就是向Acceptor发出每一个Paxos的Prepare请求。那既然我们需要一次过发出那么多个Prepare请求，**不如把这些Prepare都打包在一起，做成单独一个Prepare请求**，接下来每条记录都只用运行Phase 2即可。这也是论文中强调的Paxos高效的原因。值得注意的是，这个Prepare请求不仅是针对Log当前存在的记录，也包括Log之后无限长的记录。也就是说之后再有新的记录，如果Proposer没有更换的话，同样没必要再运行Phase 1了。如果Log中的一些记录存在着Proposer不知道的命令，根据 Basic Paxos 的原理，Acceptor会通过Promise请求把缺失的命令告诉Proposer。

我们还有一种额外情况需要处理。当拥有某一条记录的所有进程都运行失败的时候，记录对应的命令就完全消失了。新的Proposer完全没有办法得知对应的命令是什么。如图所示，紫蓝色表示记录已经被chosen以及被运行，绿色表示记录已经被chosen，但还不能被运行，因为中间缺失一条记录：

<img src="{{ site.baseurl }}/images/MultiPaxosLog.png" width="400"/>

**Figure 4: Log中存在缺失值导致数据库无法继续执行接下来的命令**

我们可以设定一条`no-op`命令，就是数据库什么都不干。当Proposer完全没办法恢复丢失的命令时，可以提出这个`no-op`命令，填补缺失的值，继续运行Log之后的命令。至于那条丢失的命令，由于已经不存在与集群中，我们只能当它没有发生过。如图所示：

<img src="{{ site.baseurl }}/images/MultiPaxosLogNoOp.png" width="400"/>

**Figure 5: 用no-op填补缺失值**

## 总结
我们学习了Paxos以及它简洁的设计，明白了如何通过扩展最基本的Paxos，得到一个可以用于状态机的 Multi-Paxos 协议。值得注意的是，Paxos只能在没有拜占庭将军问题下运行，每个Paxos节点都必须是诚实的，fail-fast的。也就是说，当错误发生时，只能停止运行，而不能把错误的结果继续报告给其他节点。但是在现实情况下，要做到真正的fail-fast是十分困难的，这也是很多一致性算法共有的问题。

## 引用材料
1. [The Part-Time Parliament](https://lamport.azurewebsites.net/pubs/lamport-paxos.pdf)
2. [Paxos Made Simple](https://lamport.azurewebsites.net/pubs/paxos-simple.pdf)
3. [Paxos Lecture](https://youtu.be/JEpsBg0AO6o)

<style type="text/css">
    .phase { list-style-type: lower-alpha; }
</style>
